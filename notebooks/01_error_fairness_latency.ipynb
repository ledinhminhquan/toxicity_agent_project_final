{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2daa9389",
   "metadata": {},
   "source": [
    "# Error Analysis + Fairness + Latency Benchmark (Privacy-Preserving)\n",
    "\n",
    "Notebook này hỗ trợ bạn **ăn điểm rubric doanh nghiệp** hơn bằng cách:\n",
    "- Error analysis (không in raw toxic text)\n",
    "- Fairness slice evaluation (identity mention heuristic)\n",
    "- Latency benchmark p50/p95/p99\n",
    "\n",
    "⚠️ Safety: dataset có thể chứa nội dung độc hại. Notebook này **không in** text từ dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d15847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 0) Controls\n",
    "USE_DRIVE = True  #@param {type:\"boolean\"}\n",
    "DRIVE_PROJECT_DIR = \"NLP_Project/toxicity_agent\"  #@param {type:\"string\"}\n",
    "\n",
    "# Paths to configs (inside repo)\n",
    "TRAIN_CONFIG = \"configs/train_final.yaml\"  #@param {type:\"string\"}\n",
    "INFER_CONFIG = \"configs/infer.yaml\"  #@param {type:\"string\"}\n",
    "FAIRNESS_CONFIG = \"configs/fairness_slices.yaml\"  #@param {type:\"string\"}\n",
    "\n",
    "THRESHOLD = 0.5  #@param {type:\"number\"}\n",
    "MAX_SAMPLES = None  #@param {type:\"raw\"}  # set int for faster run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2a0aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1) Mount Drive + set artifacts dirs\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "if USE_DRIVE:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "if USE_DRIVE:\n",
    "    ARTIFACTS_DIR = f\"/content/drive/MyDrive/{DRIVE_PROJECT_DIR}/artifacts\"\n",
    "else:\n",
    "    ARTIFACTS_DIR = \"/content/artifacts\"\n",
    "\n",
    "MODEL_DIR = os.path.join(ARTIFACTS_DIR, \"models\")\n",
    "RUN_DIR = os.path.join(ARTIFACTS_DIR, \"runs\")\n",
    "\n",
    "Path(ARTIFACTS_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(MODEL_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RUN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "os.environ[\"ARTIFACTS_DIR\"] = ARTIFACTS_DIR\n",
    "os.environ[\"TOXICITY_MODEL_DIR\"] = MODEL_DIR\n",
    "os.environ[\"TOXICITY_RUN_DIR\"] = RUN_DIR\n",
    "\n",
    "print(\"ARTIFACTS_DIR:\", ARTIFACTS_DIR)\n",
    "print(\"MODEL_DIR:\", MODEL_DIR)\n",
    "print(\"RUN_DIR:\", RUN_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847e0dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 2) Install deps (Colab-safe) + install package\n",
    "# Assumes you already have the repo in /content/toxicity_agent_project\n",
    "# If not, clone or restore it first.\n",
    "import os\n",
    "REPO_DIR = \"/content/toxicity_agent_project\"\n",
    "%cd $REPO_DIR\n",
    "\n",
    "!pip -q install --upgrade pip setuptools wheel\n",
    "!pip -q install -r requirements_colab.txt\n",
    "!pip -q install -e . --no-deps\n",
    "!pip -q check\n",
    "print(\"✅ Installed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad06b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 3) Run error analysis (privacy-preserving)\n",
    "import os\n",
    "max_arg = f\"--max-samples {MAX_SAMPLES}\" if MAX_SAMPLES is not None else \"\"\n",
    "!toxicity-agent error-analysis --config {TRAIN_CONFIG} --split test --threshold {THRESHOLD} {max_arg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad4dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 4) Run fairness slice evaluation\n",
    "import os\n",
    "max_arg = f\"--max-samples {MAX_SAMPLES}\" if MAX_SAMPLES is not None else \"\"\n",
    "!toxicity-agent fairness --config {TRAIN_CONFIG} --fairness-config {FAIRNESS_CONFIG} --split test --threshold {THRESHOLD} {max_arg}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecb65d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 5) Run latency benchmark (agent end-to-end)\n",
    "!toxicity-agent benchmark --config {INFER_CONFIG} --n 300 --warmup 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26c186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 6) Load latest reports + show summary tables (no raw text)\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "run_dir = Path(RUN_DIR)\n",
    "\n",
    "def latest_json(folder: Path, prefix: str):\n",
    "    paths = sorted(folder.glob(f\"{prefix}-*.json\"), key=lambda p: p.name)\n",
    "    return paths[-1] if paths else None\n",
    "\n",
    "err_path = latest_json(run_dir/\"error_analysis\", \"error-analysis\")\n",
    "fair_path = latest_json(run_dir/\"fairness\", \"fairness\")\n",
    "bench_path = latest_json(run_dir/\"benchmarks\", \"benchmark\")\n",
    "\n",
    "print(\"Latest error analysis:\", err_path)\n",
    "print(\"Latest fairness report:\", fair_path)\n",
    "print(\"Latest benchmark:\", bench_path)\n",
    "\n",
    "if err_path:\n",
    "    err = json.loads(err_path.read_text(encoding=\"utf-8\"))\n",
    "    rows=[]\n",
    "    for lf, cc in err.get(\"confusion_per_label\", {}).items():\n",
    "        rows.append({\"label\": lf, **cc})\n",
    "    display(pd.DataFrame(rows))\n",
    "\n",
    "if fair_path:\n",
    "    fair = json.loads(fair_path.read_text(encoding=\"utf-8\"))\n",
    "    slice_rows=[]\n",
    "    for s in fair.get(\"slices\", []):\n",
    "        slice_rows.append({\n",
    "            \"slice\": s[\"slice_name\"],\n",
    "            \"n\": s[\"n\"],\n",
    "            \"f1_micro\": s.get(\"metrics\", {}).get(\"f1_micro\"),\n",
    "            \"auc_macro\": s.get(\"metrics\", {}).get(\"auc_macro\"),\n",
    "        })\n",
    "    display(pd.DataFrame(slice_rows).sort_values(\"n\", ascending=False))\n",
    "\n",
    "if bench_path:\n",
    "    bench = json.loads(bench_path.read_text(encoding=\"utf-8\"))\n",
    "    display(pd.DataFrame([bench.get(\"stats\", {})]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
