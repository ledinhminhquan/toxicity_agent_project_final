project:
  name: toxicity-agent
  seed: 42
paths:
  data_dir: ''
  artifacts_dir: ''
dataset:
  name: thesofakillers/jigsaw-toxic-comment-classification-challenge
  text_field: comment_text
  id_field: id
  label_fields:
  - toxic
  - severe_toxic
  - obscene
  - threat
  - insult
  - identity_hate
  negative_downsample_ratio: 1.0
  max_train_samples: null
  max_eval_samples: null
split:
  train_ratio: 0.9
  val_ratio: 0.05
  test_ratio: 0.05
  seed: 42
model:
  hf_model_name: microsoft/deberta-v3-base
  max_length: 256
training:
  batch_size: 32
  eval_batch_size: 64
  learning_rate: 2.0e-05
  weight_decay: 0.01
  num_train_epochs: 4
  warmup_ratio: 0.06
  fp16: true
  gradient_accumulation_steps: 1
  logging_steps: 50
  eval_strategy: steps
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  metric_for_best_model: f1_micro
  greater_is_better: true
  bf16: true
  gradient_checkpointing: true
  early_stopping: true
  early_stopping_patience: 2
  use_pos_weight: true
tuning:
  enabled: true
  learning_rates:
  - 1.0e-05
  - 2.0e-05
  - 3.0e-05
  batch_sizes:
  - 16
  - 32
  num_train_epochs:
  - 1
  - 2
  max_trials: 6
baselines:
  tfidf_max_features: 80000
  tfidf_ngram_range:
  - 1
  - 2
  lr_C: 4.0
export:
  onnx: false
